{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fc31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcheung/.virtualenvs/thick-ML-deployment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3981a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from steps.load_step import load\n",
    "from steps.preprocess_step import preprocess\n",
    "from steps.model_step import model\n",
    "from steps import utils\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815abaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 11:12:29.400510: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=4, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x149bd00d0>: Failed to establish a new connection: [Errno 65] No route to host')': /api/2.0/mlflow/experiments/create\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=3, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x149bd0310>: Failed to establish a new connection: [Errno 65] No route to host')': /api/2.0/mlflow/experiments/create\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=2, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x149bc6d90>: Failed to establish a new connection: [Errno 65] No route to host')': /api/2.0/mlflow/experiments/create\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=1, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x149bc6fa0>: Failed to establish a new connection: [Errno 65] No route to host')': /api/2.0/mlflow/experiments/create\n"
     ]
    }
   ],
   "source": [
    "# production\n",
    "# arg parser for local\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = \"postgresql+psycopg2://postgres:postgres_pw@localhost:5435/mlflowdb\"\n",
    "# os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://localhost:9000\"\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = 'minio_user'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = \"minio_pass\"\n",
    "# os.environ[\"POSTGRES_USER\"] = 'postgres'\n",
    "# os.environ[\"POSTGRES_PASSWORD\"] = 'postgres_pw'\n",
    "# os.environ[\"POSTGRES_OPTUNA_HOSTNAME\"] = 'localhost'\n",
    "# os.environ[\"POSTGRES_OPTUNA_DB\"] = 'optunadb'\n",
    "os.environ['MLFLOW_TRACKING_URI'] = \"http://0.0.0.0:5000\"\n",
    "optuna_study_name = \"dummy-test\"\n",
    "\n",
    "# preprocess and define batch sizes for tensorflow \n",
    "ds_train = load.load_tensorflow_dataset_production('mnist')\n",
    "ds_train = ds_train.map(preprocess.preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.batch(128)\n",
    "\n",
    "# mlflow experiment\n",
    "experiment_id = utils.set_mlflow_experiment(experiment_name=optuna_study_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2dbc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c6cde65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'postgresql+psycopg2://postgres:postgres_pw@localhost:5435/mlflowdb'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c094c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load params from optuna\n",
    "optuna_storage_url=\"postgresql://{}:{}@{}:5432/{}\".format(\n",
    "            os.environ[\"POSTGRES_USER\"],\n",
    "            os.environ[\"POSTGRES_PASSWORD\"],\n",
    "            os.environ[\"POSTGRES_OPTUNA_HOSTNAME\"],\n",
    "            os.environ[\"POSTGRES_OPTUNA_DB\"]\n",
    "        )\n",
    "print('loading study...')\n",
    "study = optuna.load_study(\n",
    "    study_name=optuna_study_name,\n",
    "    storage=optuna_storage_url,\n",
    ")  \n",
    "hyperparameters = study.best_params\n",
    "\n",
    "# train model and log via mlflow\n",
    "current = pd.to_datetime('now')\n",
    "mlflow_run_name=f'production-{current.year}{current.month}{current.day}'\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id,  \n",
    "                        run_name=mlflow_run_name) as run:\n",
    "    \n",
    "    # instantiate model\n",
    "    mnist_model = model.MNIST()\n",
    "\n",
    "    # train model\n",
    "    mnist_model.fit_production(xy_tuple_train=ds_train,\n",
    "                                hyperparameters=hyperparameters)\n",
    "    # MLFlow Tracking parameters\n",
    "    mlflow.log_params(params=hyperparameters)\n",
    "\n",
    "    # MLFlow Tracking metrics \n",
    "    # Logging metrics for each epoch (housed in dictionary)\n",
    "    training_history = mnist_model._train_history.history\n",
    "    for epoch in range(0, hyperparameters['epochs']):\n",
    "        insert = {}\n",
    "        for metric, value in training_history.items():\n",
    "            insert[metric] = training_history[metric][epoch]\n",
    "        mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "    # MLFlow tracking artifact (e.g. model file)\n",
    "    # this will log the model and all its details under run_id/artifacts\n",
    "    mlflow.pyfunc.log_model(python_model=mnist_model,\n",
    "                            artifact_path='')\n",
    "\n",
    "    # Close out MLFlow run to prevent any log contamination.\n",
    "    mlflow.end_run(status='FINISHED') \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32db4af5",
   "metadata": {},
   "source": [
    "## MLFlow\n",
    "MLFLow has 4 different applications: Tracking, Registry, Models, Projects. In this script, we'll demo all 4 via a local environment. \n",
    "1. Tracking: we will log hyper-parameters used and metrics for a transfer-learning model tuned to MNIST.  \n",
    "2. Registry: after our tuned model is identified, we will register our model. \n",
    "3. Models: a stored model must be served to deliver value. Models will be leveraged for inference. \n",
    "4. Projects: DS code is packaged to reproduce runs on any platform/machine via Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mlflow.get_tracking_uri()  \n",
    "# mlflow.get_registry_uri()\n",
    "# mlflow.set_tracking_uri('http://mlflow-server.kubeflow.svc.cluster.local:5000')  \n",
    "# mlflow.set_registry_uri('http://minio.kubeflow.svc.cluster.local:9000')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "421a36b2",
   "metadata": {},
   "source": [
    "### First we'll build all the functions for our ML pipeline.\n",
    "This'll include functions for data loading, preprocessing, and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensorflow_dataset(dataset_str: str):\n",
    "    (xy_train, xy_test), ds_info = tfds.load(\n",
    "        dataset_str,\n",
    "        split=['train', 'test'], shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    return (xy_train, xy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999038d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist_tfds(image, label=None):\n",
    "    # reshape and upsample to 3 channel for transfer learning models\n",
    "    # ... for when no channel information is present\n",
    "    if len(image.shape) != 3:\n",
    "        image = np.dstack((image, image, image))\n",
    "    # ... for when channel is only 1 dimension\n",
    "    if image.shape[2] == 1:\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    # normalize pixel values\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    # resize with pad for mobilenetv2\n",
    "    image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(mlflow.pyfunc.PythonModel):     \n",
    "    def fit(self, xy_tuple_train, xy_tuple_test, hyperparameters):\n",
    "        ## Build model\n",
    "        # class names for mnist hardcoded\n",
    "        class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "        # set layer regularization for DNN\n",
    "        regularizer = tf.keras.regularizers.l1_l2(hyperparameters['l1'], hyperparameters['l2'])\n",
    "\n",
    "        # load in mobilenetv2 weights and instantiate dense classification head \n",
    "        base_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "        layers = [\n",
    "            hub.KerasLayer(\n",
    "                base_model,\n",
    "                input_shape=(224, 224, 3),\n",
    "                trainable=False,\n",
    "                name='mobilenet_embedding'),\n",
    "            tf.keras.layers.Dense(hyperparameters['num_hidden'],\n",
    "                                  kernel_regularizer=regularizer,\n",
    "                                  activation='relu',\n",
    "                                  name='dense_hidden'),\n",
    "            tf.keras.layers.Dense(len(class_names),\n",
    "                                  kernel_regularizer=regularizer,\n",
    "                                  activation='softmax',\n",
    "                                  name='mnist_prob')\n",
    "        ]\n",
    "\n",
    "        self._model = tf.keras.Sequential(layers, name='mnist-classification')\n",
    "\n",
    "        # compile model \n",
    "        self._model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                            from_logits=False),\n",
    "                            metrics=['accuracy'])\n",
    "                      \n",
    "        ## Fit model\n",
    "        # fit model and save history to model store\n",
    "        self._train_history = self._model.fit(xy_tuple_train, epochs=hyperparameters['epochs'], validation_data=xy_tuple_test)\n",
    "        self._model_base = base_model\n",
    "        \n",
    "    def predict(self, context, model_input: np.ndarray) -> np.ndarray:\n",
    "        image, _ = preprocess_mnist_tfds(model_input)\n",
    "        image = tf.reshape(image, [1, 224, 224, 3])\n",
    "        return self._model.predict(image).argmax()\n",
    "\n",
    "\n",
    "                            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d567d8d",
   "metadata": {},
   "source": [
    "### Tracking your model\n",
    "#### MLFlow has two levels for organizing projects:\n",
    "1. At the top level we have \"experiments\". These should be named as \"project-task-version\" (e.g. mnist-classification)\n",
    "2. At the lower level we have \"runs\". A run consists of logging hyperparameters and metrics when models are trained. Multiple runs can be stored within an experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow Tracking requires definition of experiment name AND logged params\n",
    "# Experiment names they should be defined as \"project-task-version\"\n",
    "experiment_name = \"mnist-classification\"\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        experiment_name,\n",
    "        tags={\"version\": \"v0.1\"},\n",
    "    )\n",
    "except mlflow.exceptions.MlflowException as e: \n",
    "    if str(e) == f\"Experiment '{experiment_name}' already exists.\":\n",
    "        print(f'Experiment already exists, setting experiment to {experiment_name}')\n",
    "        experiment_info = mlflow.set_experiment(experiment_name)\n",
    "        experiment_id = experiment_info.experiment_id\n",
    "\n",
    "experiment = mlflow.get_experiment(experiment_id)\n",
    "print(\"---------------------\")\n",
    "print('Experiment details are:')\n",
    "print(\"Name: {}\".format(experiment.name))\n",
    "print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "print(\"Tags: {}\".format(experiment.tags))\n",
    "print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "print(\"Creation timestamp: {}\".format(experiment.creation_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10969a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and define batch sizes for tensorflow \n",
    "ds_train, ds_test = load_tensorflow_dataset('mnist')\n",
    "model = MNIST()\n",
    "ds_train = ds_train.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_test = ds_test.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74397177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# log a base model \n",
    "hyperparams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "\n",
    "# Good practice to explicitly define experiment_id and run_name. \n",
    "# Experiment_id can be extracted from above. \n",
    "# Run name examples (e.g. Linear Regression Default Hyperparams).\n",
    "mlflow_run_name='MNIST Xfer Learning Base'\n",
    "with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=mlflow_run_name) as run:\n",
    "    # You can set autolog for tensorflow model.\n",
    "    # Note that autolog does not allow logging of any additional params and metrics.\n",
    "    # We'll choose to do manual logging.\n",
    "    # mlflow.tensorflow.autolog()\n",
    "\n",
    "    model.fit(ds_train, ds_test, hyperparams)\n",
    "\n",
    "    # MLFlow Tracking parameters\n",
    "    mlflow.log_params(params=hyperparams)\n",
    "    \n",
    "    # MLFlow Tracking metrics \n",
    "    # Logging metrics for each epoch (housed in dictionary)\n",
    "    training_history = model._train_history.history\n",
    "    for epoch in range(0, hyperparams['epochs']):\n",
    "        insert = {}\n",
    "        for metric, value in training_history.items():\n",
    "            insert[metric] = training_history[metric][epoch]\n",
    "        mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "    # MLFlow tracking artifact (e.g. model file)\n",
    "    # this will log the model and all its details under run_id/artifacts\n",
    "    mlflow.pyfunc.log_model(python_model=model,\n",
    "                            artifact_path=\"\")\n",
    "\n",
    "    # Close out MLFlow run to prevent any log contamination.\n",
    "    mlflow.end_run(status='FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2676a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53821fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "# hyperparameters search using Optuna\n",
    "# can scale Optuna with Kubernetes https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html\n",
    "def objective(trial): \n",
    "    \"\"\"\n",
    "    Optuna objective function for tuning transfer learning model\n",
    "    \"\"\"\n",
    "    hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.1, log=True),\n",
    "        'l1': trial.suggest_float('l1', 0.0, 0.1),\n",
    "        'l2': trial.suggest_float('l2', 0.0, 0.1),\n",
    "        'num_hidden': trial.suggest_int('num_hidden', 8, 64),\n",
    "        'epochs': trial.suggest_int('epochs', 1, 3)\n",
    "    }\n",
    "\n",
    "    model.fit(ds_train, ds_test, hyperparams)\n",
    "    training_history = model._train_history.history\n",
    "    validation_accuracy = training_history['val_accuracy'][-1]\n",
    "    return validation_accuracy\n",
    "\n",
    "POSTGRES_DB=\"deployment_demos_optuna-db_1\"\n",
    "POSTGRES_USER=\"optuna-user\"\n",
    "POSTGRES_PASSWORD=\"optuna-pass\"\n",
    "\n",
    "optuna_study_name = \"mnist-classification-test\"\n",
    "optuna_storage_url=\"postgresql://{}:{}@localhost:5433/{}\".format(\n",
    "            POSTGRES_USER,\n",
    "            POSTGRES_PASSWORD,\n",
    "            POSTGRES_DB,\n",
    "           )\n",
    "  \n",
    "# try:\n",
    "#     print('loading study...')\n",
    "#     study = optuna.load_study(\n",
    "#         study_name=optuna_study_name,\n",
    "#         storage=optuna_storage_url,\n",
    "#     )\n",
    "    \n",
    "# except KeyError:\n",
    "#     print('no study found. building from scratch...')\n",
    "#     study = optuna.create_study(\n",
    "#         study_name=optuna_study_name,\n",
    "#         storage=optuna_storage_url,\n",
    "#         pruner=optuna.pruners.HyperbandPruner(),\n",
    "#         direction='maximize')\n",
    "\n",
    "study = optuna.create_study(\n",
    "        study_name=optuna_study_name,\n",
    "        pruner=optuna.pruners.HyperbandPruner(),\n",
    "        direction='maximize')\n",
    "\n",
    "study.optimize(objective,\n",
    "               n_trials=8,\n",
    "               n_jobs=2,\n",
    "              callbacks=[MLflowCallback(metric_name=\"val_accuracy\")]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f915bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the hyper parameter tuned model \n",
    "# No need to set experiment ID if you've set above. \n",
    "# Set a run name here(e.g. Linear Regression Default Hyperparams).\n",
    "mlflow_run_name='MNIST Xfer Learning Hyperparam tuned'\n",
    "with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=mlflow_run_name) as run:\n",
    "    \n",
    "    model.train(ds_train, ds_test, hyperparams)\n",
    "\n",
    "    # MLFlow Tracking parameters\n",
    "    mlflow.log_params(params=hyperparams)\n",
    "    \n",
    "    # MLFlow Tracking metrics \n",
    "    # Logging metrics for each epoch (housed in dictionary)\n",
    "    training_history = model._train_history.history\n",
    "    for epoch in range(0, hyperparams['epochs']):\n",
    "        insert = {}\n",
    "        for metric, value in training_history.items():\n",
    "            insert[metric] = training_history[metric][epoch]\n",
    "        mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "    # MLFlow tracking artifact (e.g. model file)\n",
    "    # mlflow.log_artifact(self._model)\n",
    "\n",
    "    # we need to define how we use tags for testing or if we even need them...\n",
    "    mlflow.set_tag(key=\"test\",\n",
    "                   value=\"manual-logging\")\n",
    "\n",
    "    mlflow.end_run(status='FINISHED')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54fe297d",
   "metadata": {},
   "source": [
    "### Registering your model\n",
    "Once you've run a couple experiments (e.g. hyper parameter tuning) we can select the best model and register that. Model registering just means that we decide this our optimized model to save. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ec055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search runs \n",
    "filtering_clause = 'params.epochs = \"10\" and params.learning_rate = \"0.01\"'\n",
    "run = mlflow.search_runs(\n",
    "    experiment_names=['mnist-classification'],\n",
    "    filter_string=filtering_clause,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.val_accuracy DESC\"],\n",
    ")\n",
    "\n",
    "# best performing model run_id. This'll be used to register our model\n",
    "best_run_id = run['run_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a8f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model after a scanning a couple different runs from your experiment \n",
    "model_name = f'{experiment.name}'\n",
    "# mlflow.tensorflow.log_model\n",
    "mv = mlflow.register_model(model_uri=f\"runs:/{best_run_id}/\",\n",
    "                           name=model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b5c3143",
   "metadata": {},
   "source": [
    "### Serving model\n",
    "After we've registered our model, we can now test inference for that model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9493c05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load directly from registry \n",
    "# model_version=1\n",
    "# model_name = f'{experiment.name}'\n",
    "# model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "# load using artifact path directory\n",
    "results = mlflow.search_registered_models(filter_string='name = \"mnist-classification\"')\n",
    "latest_model_details = results[0].latest_versions[0]\n",
    "model = mlflow.pyfunc.load_model(model_uri=f'{latest_model_details.source[7:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample and load a MNIST JPEG\n",
    "import os \n",
    "from random import sample\n",
    "from PIL import Image\n",
    "fp = '/Users/jcheung/Documents/GitHub/thin-ML-deployment/app/ml/test_images'\n",
    "files = [f'{fp}/{x}' for x in os.listdir(fp) if x.split('.')[-1] == 'jpg']\n",
    "filename = sample(files, 1)[0]\n",
    "image = np.array(Image.open(filename))\n",
    "\n",
    "# predict using custom mlflow model\n",
    "predicted = model.predict(image)\n",
    "\n",
    "# output and compare results\n",
    "true = filename.split('/')[-1].split('_')[-1][0]\n",
    "\n",
    "print(f'True:{true} and predicted:{predicted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e10b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kubeflow.katib as katib\n",
    "\n",
    "# Step 1. Create an objective function.\n",
    "def objective(hyperparams):\n",
    "    training_history = model.train(ds_train, ds_test, hyperparams)\n",
    "    validation_accuracy = training_history.history['val_accuracy'][-1]\n",
    "    # Katib parses metrics in this format: <metric-name>=<metric-value>.\n",
    "    print(f\"result={result}\")\n",
    "\n",
    "# Step 2. Create HyperParameter search space.\n",
    "hyperparams = {\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.1, log=True),\n",
    "    'l1': trial.suggest_float('l1', 0.0, 1),\n",
    "    'l2': trial.suggest_float('l2', 0.0, 1),\n",
    "    'num_hidden': katib.search.int(min=8, max=64),\n",
    "    'epochs': katib.search.int(min=5, max=10)\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    \"a\": katib.search.int(min=10, max=20),\n",
    "    \"b\": katib.search.double(min=0.1, max=0.2)\n",
    "}\n",
    "\n",
    "# Step 3. Create Katib Experiment.\n",
    "katib_client = katib.KatibClient()\n",
    "name = \"tune-experiment\"\n",
    "katib_client.tune(\n",
    "    name=name,\n",
    "    objective=objective,\n",
    "    parameters=parameters,\n",
    "    objective_metric_name=\"result\",\n",
    "    max_trial_count=12\n",
    ")\n",
    "\n",
    "# Step 4. Get the best HyperParameters.\n",
    "print(katib_client.get_optimal_hyperparameters(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thick-ML-deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
